{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\DEVANSH\n",
      "[nltk_data]     JAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\DEVANSH\n",
      "[nltk_data]     JAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import *\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftamil=pd.read_csv('tamil_train.csv')\n",
    "dfmal=pd.read_csv('mal_train.csv')\n",
    "dfkannada=pd.read_csv('kannada_train.csv')\n",
    "dftamiltest=pd.read_csv('tamil_test.csv')\n",
    "dfmaltest=pd.read_csv('mal_test.csv')\n",
    "dfkannadatest=pd.read_csv('kannada_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35138, 2)\n",
      "(6216, 2)\n",
      "(16010, 2)\n",
      "(4391, 9)\n",
      "(777, 1)\n",
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dftamil.shape)\n",
    "print(dfkannada.shape)\n",
    "print(dfmal.shape)\n",
    "print(dftamiltest.shape)\n",
    "print(dfkannadatest.shape)\n",
    "print(dfmaltest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Table Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dftamiltest=dftamiltest.drop(['Unnamed: 1','Unnamed: 2','Unnamed: 3','Unnamed: 4','Unnamed: 5','Unnamed: 6','Unnamed: 7','Unnamed: 8'], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'എന്ത് ഊള എഡിറ്റിംഗ് ആടോ ഇത് ഒരുമാതിരി vivo videoorderil ചെയ്തപോലെ'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmaltest['Fefka ee padam release cheyyan samadhicho? '][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft=pd.DataFrame({\"movie vara level la Erika poguthu\":[\"movie vara level la Erika poguthu\"],\n",
    "                 \"Not_offensive\":[\"Not_offensive\"]})\n",
    "dfk=pd.DataFrame({\"Tik tok alli jagala madtidralla adra baggenu ondu video madi anna super agi ugididdira\":[\"Tik tok alli jagala madtidralla adra baggenu ondu video madi anna super agi ugididdira\"],\n",
    "                 \"Not_offensive\":[\"Not_offensive\"]})\n",
    "dfttest=pd.DataFrame({\"14.12.2018 epo trailer pathutu irken ... Semaya iruku\":[\"14.12.2018 epo trailer pathutu irken ... Semaya iruku\"]})\n",
    "dfktest=pd.DataFrame({\"Anna nim e vedio nodinu mathe chaina apps use madidre...nijakku avru maryade  swabhiman ildoru bharatha dalli irbardu ....antavrnella nimhange maryade thegibeku..... \":[\"Anna nim e vedio nodinu mathe chaina apps use madidre...nijakku avru maryade  swabhiman ildoru bharatha dalli irbardu ....antavrnella nimhange maryade thegibeku..... \"]})\n",
    "dfmtest=pd.DataFrame({\"Fefka ee padam release cheyyan samadhicho? \":[\"Fefka ee padam release cheyyan samadhicho? \"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datat=dft.append(dftamil, ignore_index = True)\n",
    "datak=dfk.append(dfkannada, ignore_index = True)\n",
    "datatt=dfttest.append(dftamiltest, ignore_index = True)\n",
    "datakt=dfktest.append(dfkannadatest, ignore_index = True)\n",
    "datamt=dfmtest.append(dfmaltest, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tamil=datat.rename(columns={\"movie vara level la Erika poguthu\": \"text\", \"Not_offensive\": \"label\"})\n",
    "data_kannada=datak.rename(columns={\"Tik tok alli jagala madtidralla adra baggenu ondu video madi anna super agi ugididdira\": \"text\", \"Not_offensive\": \"label\"})\n",
    "data_tamiltest=datatt.rename(columns={\"14.12.2018 epo trailer pathutu irken ... Semaya iruku\": \"text\"})\n",
    "data_kannadatest=datakt.rename(columns={\"Anna nim e vedio nodinu mathe chaina apps use madidre...nijakku avru maryade  swabhiman ildoru bharatha dalli irbardu ....antavrnella nimhange maryade thegibeku..... \": \"text\"})\n",
    "data_maltest=datamt.rename(columns={\"Fefka ee padam release cheyyan samadhicho? \": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35139, 2)\n",
      "(6217, 2)\n",
      "(16010, 2)\n",
      "(4392, 1)\n",
      "(778, 1)\n",
      "(2001, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data_tamil.shape)\n",
    "print(data_kannada.shape)\n",
    "print(dfmal.shape)\n",
    "print(data_tamiltest.shape)\n",
    "print(data_kannadatest.shape)\n",
    "print(data_maltest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafinal=pd.concat([data_tamil,data_kannada,dfmal])\n",
    "datafinaltest=pd.concat([data_tamiltest,data_kannadatest,data_maltest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.12.2018 epo trailer pathutu irken ... Semay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paka thana poro movie la Enna irukunu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“U kena tunggu lebih lama lagi untuk tahu saya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suriya anna vera level anna mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suma kaththaatha da sound over a pooda kudaath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ஆனவக் கொலைகள் நடக்காவிடில் ...நம் அடையாளம் அளி...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>teasere intha alavukk masss aa irunthal padam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Enthanai kudumbam velila sollamudiyama sethuku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Power ranger megazord niyabagam vantha hit like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Parkavakula Udayar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Part 2 எடுத்து எடுத்து நல்ல படைப்பான part 1 ன ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Na oru 1000 times mela pathu irukn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1st Ayan super hit 2nd maatran mega hit  3rd k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0:28 Shaktimaan Parle-G Biscuit SaptuNaanum si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>தேவர் சமுதாய மக்கள் சார்பாக படம் வெற்றி பெற வா...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Thalaivaaa.......nee ena periya vasul Manan ah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Enna da idhu marana mass ah irukku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Gownder inam sarapaka padam vetriper valthukala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>செஞ்சி கோட்டை கவுண்டர் சார்பாக படம் வெற்றி பெற...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.1M VIEWS NOW 16M VIEWS SOON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>எவனுக்கும் அடங்க மாட்டோம்... எவளுக்கும் மயங்க ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Movie yachum nailla eruka nu papom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nice aana teaser starting laa vandha dialogue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Indheee oru padam poodhum thalaivaaaa.... Naaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Yannada trailer idhu yadho oruthava family la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ajith = h.Vinod= Siva = h. Raja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2M likes varumnu solravanga Like podunga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Dai unmaiyanumea marana kattu katuvega pola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Petta movie pakka Yarellam Wait Panringa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>இந்தப் படத்தை இப்ப பேசுவதை விட பத்து வருஷம் அப...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>Kandu kuru pottunavark kanji kudichu Mari nilk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>Kidukkii mammukaaa kidukii odukathe mass bgm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>Feels like it will be a slow movie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>Ikkha Mess aanu  #mamookkha  #dq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>nammude jeevithathil nadanna karyangal okke th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>പ്രവചിക്കുന്നു.  .  പടം പൊട്ടും ബേയ്</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>padam nallathanenkil ellavarum theateril thann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>daivame chadichooo copy sundaran analloooo  in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>Ikka polichu but Gopi ഏട്ടന്റെ bgm അത്ര പോര ..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>VADAA aaa dialog vera level ane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>Abudabi delma malil vannavar like adiku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>കൊളഗണ്ടിലെ ഇറുക്കെ.. കൊല്ലാമെ വിടമാട്ടെ   SHYL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>```HIGH VOLTAGE INSIDE.. UNAUTHORIZED PERSONS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>ഇത് നയൻതാരയ്ക്ക് വെച്ച റോൾ ആയിരുന്നില്ലേ   Any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>open for online job for students</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>Like a tamil and Telugu movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>Guys ente channel onnu subscribe cheyyaamo 🥰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>Katta waiting for mamangam  Mammookkaaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>ഇപ്പോൾ വന്നത് ക്‌ളാസ് ഇനി വരാനുള്ളത് ഒരു ഒന്ന ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>0:39 ഇതൊക്ക എവിടെ ജനിച്ചത് ആണാവോ???  ചൈനേല്</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>ആമയെ വെള്ളത്തിൽ മുക്കി കൊല്ലുന്ന ടീം aanuuuuu....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>ട്രെയിലർ കണ്ടു ഞെട്ടിപ്പോയി</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>Wah...Wah..ഇതെന്താ അഭിനയിക്കാൻ പറഞ്ഞാൽ ജീവിച്ച...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>എജ്ജാതി ഹൌ ബല്ലാത്ത ജാതി .എന്തൊരു സ്ക്രീൻ പ്രെ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>ഫുട്ബാൾ വിഡിയോകൾക്കായി ചാനൽ സബ്സ്ക്രൈബ് ചെയ്യൂ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Swargatthil ninnu purathaakkappetta daivatthin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Ivide Palakkad Jayettan Fans club nnu ashamsak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>ഈ പടത്തിന് വെയിറ്റ് ചെയ്യുന്ന മമ്മൂക്ക ഫാൻസും</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>കട്ട ലാലേട്ടൻ ഫാൻസ് ഒരു ലൈക് തന്നിട്ട് പോവാമോ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>Koora padam urappa kandal aryam.. Hello</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7171 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     14.12.2018 epo trailer pathutu irken ... Semay...\n",
       "1                 Paka thana poro movie la Enna irukunu\n",
       "2     “U kena tunggu lebih lama lagi untuk tahu saya...\n",
       "3                      Suriya anna vera level anna mass\n",
       "4     suma kaththaatha da sound over a pooda kudaath...\n",
       "5     ஆனவக் கொலைகள் நடக்காவிடில் ...நம் அடையாளம் அளி...\n",
       "6     teasere intha alavukk masss aa irunthal padam ...\n",
       "7     Enthanai kudumbam velila sollamudiyama sethuku...\n",
       "8       Power ranger megazord niyabagam vantha hit like\n",
       "9                                    Parkavakula Udayar\n",
       "10    Part 2 எடுத்து எடுத்து நல்ல படைப்பான part 1 ன ...\n",
       "11                   Na oru 1000 times mela pathu irukn\n",
       "12    1st Ayan super hit 2nd maatran mega hit  3rd k...\n",
       "13    0:28 Shaktimaan Parle-G Biscuit SaptuNaanum si...\n",
       "14    தேவர் சமுதாய மக்கள் சார்பாக படம் வெற்றி பெற வா...\n",
       "15    Thalaivaaa.......nee ena periya vasul Manan ah...\n",
       "16                   Enna da idhu marana mass ah irukku\n",
       "17      Gownder inam sarapaka padam vetriper valthukala\n",
       "18    செஞ்சி கோட்டை கவுண்டர் சார்பாக படம் வெற்றி பெற...\n",
       "19                       15.1M VIEWS NOW 16M VIEWS SOON\n",
       "20    எவனுக்கும் அடங்க மாட்டோம்... எவளுக்கும் மயங்க ...\n",
       "21                   Movie yachum nailla eruka nu papom\n",
       "22    nice aana teaser starting laa vandha dialogue ...\n",
       "23    Indheee oru padam poodhum thalaivaaaa.... Naaa...\n",
       "24    Yannada trailer idhu yadho oruthava family la ...\n",
       "25                      Ajith = h.Vinod= Siva = h. Raja\n",
       "26             2M likes varumnu solravanga Like podunga\n",
       "27          Dai unmaiyanumea marana kattu katuvega pola\n",
       "28             Petta movie pakka Yarellam Wait Panringa\n",
       "29    இந்தப் படத்தை இப்ப பேசுவதை விட பத்து வருஷம் அப...\n",
       "...                                                 ...\n",
       "1971  Kandu kuru pottunavark kanji kudichu Mari nilk...\n",
       "1972       Kidukkii mammukaaa kidukii odukathe mass bgm\n",
       "1973                Feels like it will be a slow movie.\n",
       "1974                   Ikkha Mess aanu  #mamookkha  #dq\n",
       "1975  nammude jeevithathil nadanna karyangal okke th...\n",
       "1976               പ്രവചിക്കുന്നു.  .  പടം പൊട്ടും ബേയ്\n",
       "1977  padam nallathanenkil ellavarum theateril thann...\n",
       "1978  daivame chadichooo copy sundaran analloooo  in...\n",
       "1979     Ikka polichu but Gopi ഏട്ടന്റെ bgm അത്ര പോര ..\n",
       "1980                    VADAA aaa dialog vera level ane\n",
       "1981            Abudabi delma malil vannavar like adiku\n",
       "1982  കൊളഗണ്ടിലെ ഇറുക്കെ.. കൊല്ലാമെ വിടമാട്ടെ   SHYL...\n",
       "1983  ```HIGH VOLTAGE INSIDE.. UNAUTHORIZED PERSONS ...\n",
       "1984  ഇത് നയൻതാരയ്ക്ക് വെച്ച റോൾ ആയിരുന്നില്ലേ   Any...\n",
       "1985                   open for online job for students\n",
       "1986                      Like a tamil and Telugu movie\n",
       "1987       Guys ente channel onnu subscribe cheyyaamo 🥰\n",
       "1988            Katta waiting for mamangam  Mammookkaaa\n",
       "1989  ഇപ്പോൾ വന്നത് ക്‌ളാസ് ഇനി വരാനുള്ളത് ഒരു ഒന്ന ...\n",
       "1990        0:39 ഇതൊക്ക എവിടെ ജനിച്ചത് ആണാവോ???  ചൈനേല്\n",
       "1991  ആമയെ വെള്ളത്തിൽ മുക്കി കൊല്ലുന്ന ടീം aanuuuuu....\n",
       "1992                        ട്രെയിലർ കണ്ടു ഞെട്ടിപ്പോയി\n",
       "1993  Wah...Wah..ഇതെന്താ അഭിനയിക്കാൻ പറഞ്ഞാൽ ജീവിച്ച...\n",
       "1994  എജ്ജാതി ഹൌ ബല്ലാത്ത ജാതി .എന്തൊരു സ്ക്രീൻ പ്രെ...\n",
       "1995     ഫുട്ബാൾ വിഡിയോകൾക്കായി ചാനൽ സബ്സ്ക്രൈബ് ചെയ്യൂ\n",
       "1996  Swargatthil ninnu purathaakkappetta daivatthin...\n",
       "1997  Ivide Palakkad Jayettan Fans club nnu ashamsak...\n",
       "1998      ഈ പടത്തിന് വെയിറ്റ് ചെയ്യുന്ന മമ്മൂക്ക ഫാൻസും\n",
       "1999  കട്ട ലാലേട്ടൻ ഫാൻസ് ഒരു ലൈക് തന്നിട്ട് പോവാമോ ...\n",
       "2000            Koora padam urappa kandal aryam.. Hello\n",
       "\n",
       "[7171 rows x 1 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafinaltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = datafinal.sample(frac=1).reset_index(drop=True) #shuffling\n",
    "df_test = datafinaltest.sample(frac=1).reset_index(drop=True) #shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7171, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical value given to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "df_train['Label']=labelencoder.fit_transform(df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kajal  Fans Subscribe</td>\n",
       "      <td>Not_offensive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Enda padam muluka ipdiye kathuviya.. ada saami...</td>\n",
       "      <td>Offensive_Targeted_Insult_Individual</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yogi sema Vijay mathiriye apdeye mimicry pandr...</td>\n",
       "      <td>Not_offensive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUCH A LOVELY ACTING THALA , LOVE U</td>\n",
       "      <td>Not_offensive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>மயிருக்கு கருமை தரும் கறிவேப்பிலையின் கருமை தம...</td>\n",
       "      <td>Not_offensive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                              Kajal  Fans Subscribe   \n",
       "1  Enda padam muluka ipdiye kathuviya.. ada saami...   \n",
       "2  Yogi sema Vijay mathiriye apdeye mimicry pandr...   \n",
       "3                SUCH A LOVELY ACTING THALA , LOVE U   \n",
       "4  மயிருக்கு கருமை தரும் கறிவேப்பிலையின் கருமை தம...   \n",
       "\n",
       "                                  label  Label  \n",
       "0                         Not_offensive      0  \n",
       "1  Offensive_Targeted_Insult_Individual      2  \n",
       "2                         Not_offensive      0  \n",
       "3                         Not_offensive      0  \n",
       "4                         Not_offensive      0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    43122\n",
       "4     3309\n",
       "2     3069\n",
       "1     3026\n",
       "5     1522\n",
       "6     1454\n",
       "7     1287\n",
       "3      577\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57366,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=[]\n",
    "y=df_train['Label'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1dd79f887b8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['Label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.apply(lambda x: x.astype(str).str.lower()) #Lower case\n",
    "df_test = df_test.apply(lambda x: x.astype(str).str.lower()) #Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text']=df_train['text'].str.replace('[^\\s\\w]','') #removing punctuations\n",
    "df_test['text']=df_test['text'].str.replace('[^\\s\\w]','') #removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text_token'] = df_train['text'].apply(lambda x: word_tokenize(x)) #sentence converted to words\n",
    "df_test['text_token'] = df_test['text'].apply(lambda x: word_tokenize(x)) #sentence converted to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() \n",
    "df_train['text'] = df_train['text_token'].apply(lambda x: list(ps.stem(i) for i in x)) #Stemming\n",
    "df_test['text'] = df_test['text_token'].apply(lambda x: list(ps.stem(i) for i in x)) #Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "df_train['text'] = df_train['text'].apply(lambda x: ' '.join(list(i for i in x if i not in stops))) #removind all the stopwords\n",
    "df_test['text'] = df_test['text'].apply(lambda x: ' '.join(list(i for i in x if i not in stops))) #removind all the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\DEVANSH\n",
      "[nltk_data]     JAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df_train['text'] =df_train['text'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "df_test['text'] =df_test['text'].apply(lambda x: lemmatizer.lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kajal fan subscrib</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>0</td>\n",
       "      <td>[kajal, fans, subscribe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enda padam muluka ipdiy kathuviya ada saami na...</td>\n",
       "      <td>offensive_targeted_insult_individual</td>\n",
       "      <td>2</td>\n",
       "      <td>[enda, padam, muluka, ipdiye, kathuviya, ada, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yogi sema vijay mathiriy apdey mimicri pandra ...</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>0</td>\n",
       "      <td>[yogi, sema, vijay, mathiriye, apdeye, mimicry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love act thala love u</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>0</td>\n",
       "      <td>[such, a, lovely, acting, thala, love, u]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>மயரகக கரம தரம கறவபபலயன கரம தமழ மககனங கயரகக அஞச...</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>0</td>\n",
       "      <td>[மயரகக, கரம, தரம, கறவபபலயன, கரம, தமழ, மககனங, க...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                 kajal fan subscrib   \n",
       "1  enda padam muluka ipdiy kathuviya ada saami na...   \n",
       "2  yogi sema vijay mathiriy apdey mimicri pandra ...   \n",
       "3                              love act thala love u   \n",
       "4  மயரகக கரம தரம கறவபபலயன கரம தமழ மககனங கயரகக அஞச...   \n",
       "\n",
       "                                  label Label  \\\n",
       "0                         not_offensive     0   \n",
       "1  offensive_targeted_insult_individual     2   \n",
       "2                         not_offensive     0   \n",
       "3                         not_offensive     0   \n",
       "4                         not_offensive     0   \n",
       "\n",
       "                                          text_token  \n",
       "0                           [kajal, fans, subscribe]  \n",
       "1  [enda, padam, muluka, ipdiye, kathuviya, ada, ...  \n",
       "2  [yogi, sema, vijay, mathiriye, apdeye, mimicry...  \n",
       "3          [such, a, lovely, acting, thala, love, u]  \n",
       "4  [மயரகக, கரம, தரம, கறவபபலயன, கரம, தமழ, மககனங, க...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>samma song mudiyav mudiyathu bro paaaafrom tha...</td>\n",
       "      <td>[samma, song, mudiyave, mudiyathu, bro, paaaaf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130 la vara dialogu mass</td>\n",
       "      <td>[130, the, la, vara, dialogue, mass]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like kaappaan ngk</td>\n",
       "      <td>[i, like, kaappaan, more, than, ngk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>director ranjith pardhu sakattum avan mattum s...</td>\n",
       "      <td>[director, ranjith, pardhu, sakattum, avan, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teaser la iruntha yedho onnu trailer la miss a...</td>\n",
       "      <td>[teaser, la, iruntha, yedho, onnu, trailer, la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  samma song mudiyav mudiyathu bro paaaafrom tha...   \n",
       "1                           130 la vara dialogu mass   \n",
       "2                                  like kaappaan ngk   \n",
       "3  director ranjith pardhu sakattum avan mattum s...   \n",
       "4  teaser la iruntha yedho onnu trailer la miss a...   \n",
       "\n",
       "                                          text_token  \n",
       "0  [samma, song, mudiyave, mudiyathu, bro, paaaaf...  \n",
       "1               [130, the, la, vara, dialogue, mass]  \n",
       "2               [i, like, kaappaan, more, than, ngk]  \n",
       "3  [director, ranjith, pardhu, sakattum, avan, ma...  \n",
       "4  [teaser, la, iruntha, yedho, onnu, trailer, la...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_train['text'].values\n",
    "z=df_test['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(c_train,c_test):\n",
    "    tweet_tokenizer = TweetTokenizer() \n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize, max_features = 1010)\n",
    "    Xy_train = vectorizer.fit_transform(c_train).toarray()\n",
    "    Xz_test = vectorizer.fit_transform(c_test).toarray()\n",
    "    return Xy_train,Xztest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xz_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-fd6c2b1b35eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mXy_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXz_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mXy_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXz_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-e80096e46af6>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(c_train, c_test)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mXy_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mXz_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mXz_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Xz_train' is not defined"
     ]
    }
   ],
   "source": [
    "Xy_train,Xz_test = tokenize(y,z)\n",
    "Xy_train.shape,Xz_test.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Bgm is good'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-208fa518e5d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'liblinear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXz_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Bgm is good'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\n",
    "model.fit(Xz_train, df_train['Label'])\n",
    "y_pred = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
